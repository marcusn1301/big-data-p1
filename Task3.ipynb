{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bbe62928-01d3-402d-9e45-19d009a4639d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Names of people in the group\n",
    "\n",
    "Please write the names of the people in your group in the next cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15e1606f-bbea-4e98-9f90-bf0a15da5391",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Marcus Stephan Nordal\n",
    "\n",
    "Silviu Mihai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95475f24-a272-4302-ae31-9d302e918ef7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[33mWARNING: You are using pip version 21.2.4; however, version 23.3.2 is available.\r\nYou should consider upgrading via the '/local_disk0/.ephemeral_nfs/envs/pythonEnv-3b1c31dd-a7dd-4316-ac59-cc8f8cf85ff2/bin/python -m pip install --upgrade pip' command.\u001B[0m\r\n"
     ]
    }
   ],
   "source": [
    "# We need to install 'ipython_unittest' to run unittests in a Jupyter notebook\n",
    "!pip install -q ipython_unittest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "389ff132-7e3c-444e-a980-6490e3448153",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Loading modules that we need\n",
    "from pyspark.sql.dataframe import DataFrame\n",
    "from collections import Counter\n",
    "from pyspark.sql.functions import desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b384d61-f63a-4c3f-9d87-63e0427c5ecd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# A helper function to load a table (stored in Parquet format) from DBFS as a Spark DataFrame \n",
    "def load_df(table_name: \"name of the table to load\") -> DataFrame:\n",
    "    return spark.read.parquet(table_name)\n",
    "\n",
    "users_df = load_df(\"/user/hive/warehouse/users\")\n",
    "posts_df = load_df(\"/user/hive/warehouse/posts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a34dda8-b217-4958-993e-b10e5c4c7e01",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Subtask 1: implementing two functions\n",
    "Implement these two functions:\n",
    "1. 'compute_pearsons_r' that receives a DataFrame and two column names and returns the [Pearson correlation coefficient](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient) between values of two columns;\n",
    "2. 'make_tag_graph' that in the input receives the DataFrame containing the records related to 'questions' and returns a DataFrame with two columns 'u' and 'v'; the record for row i from the resulting DataFrame is a tuple (u_i, v_i). u_i and v_j are distinct tags and have appeared together for a question.\n",
    "\n",
    "Please note that you should implement the 'compute_pearsons_r' yourself, so you should not use the 'DataFrame.stat.corr' method. Nevertheless, you can use 'DataFrame.stat.corr' to verify the correctness of your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5309637d-1430-4727-8ece-e8ce2865fae9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5218109904695661\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as Func\n",
    "\n",
    "def compute_pearsons_r(df: \"a DataFrame\", col1: \"name of column A\", col2: \"name of column B\") -> float:\n",
    "    mean1 = df.agg(Func.mean(col1)).first()[0]\n",
    "    mean2 = df.agg(Func.mean(col2)).first()[0]\n",
    "\n",
    "    squared_deviation1 = (Func.sum(Func.pow(Func.col(col1) - mean1, 2)))\n",
    "    squared_deviation2 = (Func.sum(Func.pow(Func.col(col2) - mean2, 2)))\n",
    "    value_count1 = (Func.count(Func.col(col1)) - 1)\n",
    "    value_count2 = (Func.count(Func.col(col2)) - 1)\n",
    "\n",
    "    variance1 = df.agg(squared_deviation1 / value_count1).first()[0]\n",
    "    variance2 = df.agg(squared_deviation2 / value_count2).first()[0]\n",
    "    col_product = (Func.sum(Func.col(col1) * Func.col(col2)))\n",
    "    \n",
    "    covariance = df.agg(col_product / value_count1).first()[0] - mean1 * mean2\n",
    "\n",
    "    return covariance / ((variance1 ** 0.5) * (variance2 ** 0.5))\n",
    "\n",
    "##Test\n",
    "result = compute_pearsons_r(users_df, \"Reputation\", \"UpVotes\")\n",
    "print(result)\n",
    "\n",
    "def make_tag_graph(df: \"DataFrame containing question data\") -> DataFrame:\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a81ba4f-9bfa-40b2-b5bb-e5797110b507",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Subtask 2: implementing three functions\n",
    "Impelment these three functions:\n",
    "1. 'get_nodes' that, given the result from execution of 'make_tag_graph', returns a DataFrame with one column named 'id' that includes the tags that have appeared in the tag graph;\n",
    "2. 'get_edges' that, given the result from execution of 'make_tag_graph', returns a DataFrame with two columns 'src' and 'dst' where 'src' is the source node and 'dst' is the destination node.\n",
    "\n",
    "\n",
    "Note that the term 'tag graph' in this context refers to the DataFrame reuturned by executing 'make_tag_graph'. Furthermore, 'src' and 'dst' are distinct, so 'src' != 'dst'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9b4472a-f1a1-46a5-bf84-a74fc5522749",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_nodes(df: \"DataFrame of the tag graph\") -> DataFrame:\n",
    "  ## To-do!\n",
    "  \n",
    "\n",
    "def get_edges(df: \"DataFrame of the tag graph\") -> DataFrame:\n",
    "  ## To-do!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5db7dc5-c730-4c11-b5c0-f846257649e7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Loading 'ipython_unittest' so we can use '%%unittest_main' magic command\n",
    "%load_ext ipython_unittest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26060642-7e52-4723-98ea-3b8730753439",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Subtask 3: validating the implementation by running the tests\n",
    "\n",
    "Run the cell below and make sure that all the tests run successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0bb43bc9-1eb4-4b8d-ae5a-36f1a371688d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/unittest.status+json": {
       "color": "yellow",
       "message": "",
       "previous": 0
      },
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5218109904695661\n0.14735501007545418\n0.4753227874233796\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/unittest.status+json": {
       "color": "lightgreen",
       "message": "...\n----------------------------------------------------------------------\nRan 3 tests in 9.849s\n\nOK\n",
       "previous": 0
      },
      "text/plain": [
       "Success"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...\n----------------------------------------------------------------------\nRan 3 tests in 9.849s\n\nOK\nOut[62]: <unittest.runner.TextTestResult run=3 errors=0 failures=0>"
     ]
    }
   ],
   "source": [
    "%%unittest_main\n",
    "class TestTask3(unittest.TestCase):\n",
    "  \n",
    "  error_threshold = 0.03\n",
    "  \n",
    "  def test_corr1(self):\n",
    "    # Pearson correlation coefficient between 'user reputation' and 'upvotes' received by users\n",
    "    result = compute_pearsons_r(users_df, \"Reputation\", \"UpVotes\")\n",
    "    self.assertLessEqual(abs(result-0.5218138310114108), self.error_threshold)\n",
    "    print(result)\n",
    "  \n",
    "  def test_corr2(self):\n",
    "    # Pearson correlation coefficient between 'user reputation' and 'downvotes' received by users\n",
    "    result = compute_pearsons_r(users_df, \"Reputation\", \"DownVotes\")\n",
    "    self.assertLessEqual(abs(result-0.1473558141546844), self.error_threshold)\n",
    "    print(result)\n",
    "\n",
    "  def test_corr3(self):\n",
    "    # Pearson correlation coefficient between 'question score' and the 'number of answers' it received\n",
    "    result = compute_pearsons_r(posts_df[posts_df[\"PostTypeId\"] == 1], \"Score\", \"AnswerCount\")\n",
    "    self.assertLessEqual(abs(result-0.47855272641249674), self.error_threshold)\n",
    "    print(result)\n",
    "    \n",
    "  def test_make_tag_graph(self):\n",
    "    result = make_tag_graph(df=posts_df[posts_df[\"PostTypeId\"] == 1])\n",
    "    self.assertIsInstance(result, DataFrame)\n",
    "    \n",
    "    coulmn_names = Counter(map(str.lower, ['u', 'v']))\n",
    "    self.assertCountEqual(coulmn_names, Counter(map(str.lower, result.columns)), \"Missing column(s) or column name mismatch\")\n",
    "    \n",
    "    display(result)\n",
    "    \n",
    "    self.assertEqual(result.count(), 228830)\n",
    "    \n",
    "  def test_get_nodes(self):\n",
    "    result = make_tag_graph(df=posts_df[posts_df[\"PostTypeId\"] == 1])\n",
    "    n = get_nodes(result)\n",
    "    self.assertEqual(n.count(), 638)\n",
    "    n.show()\n",
    "\n",
    "  def test_get_edges(self):\n",
    "    result = make_tag_graph(df=posts_df[posts_df[\"PostTypeId\"] == 1])\n",
    "    e = get_edges(result)\n",
    "    \n",
    "    coulmn_names = Counter(map(str.lower, ['src', 'dst']))\n",
    "    self.assertCountEqual(coulmn_names, Counter(map(str.lower, e.columns)), \"Missing column(s) or column name mismatch\")\n",
    "    \n",
    "    self.assertEqual(e.count(), 225290)\n",
    "    e.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89df9a4d-63c0-44e4-87d7-93602ec156a7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Subtask 4: answering to questions about Spark related concepts\n",
    "\n",
    "Please write a short description for the terms below---one to two short paragraphs for each term. Don't copy-paste; instead, write your own understanding.\n",
    "\n",
    "1. What do the terms 'User-Defined Functions (UDFs)', 'Data Locality', 'Bucketing', 'Distributed Filesystem' mean in the context of Spark?\n",
    "\n",
    "Write your descriptions in the next cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa3a29e6-80ad-4105-a8f9-dae8d991c6c6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Your answers..."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Task3",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

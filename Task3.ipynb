{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bbe62928-01d3-402d-9e45-19d009a4639d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Names of people in the group\n",
    "\n",
    "Please write the names of the people in your group in the next cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15e1606f-bbea-4e98-9f90-bf0a15da5391",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Marcus Stephan Nordal\n",
    "\n",
    "Silviu Mihai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95475f24-a272-4302-ae31-9d302e918ef7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 23.3.2 is available.\r\n",
      "You should consider upgrading via the '/local_disk0/.ephemeral_nfs/envs/pythonEnv-acb044fd-074b-4c09-a992-f4a672226a5e/bin/python -m pip install --upgrade pip' command.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "# We need to install 'ipython_unittest' to run unittests in a Jupyter notebook\n",
    "!pip install -q ipython_unittest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "389ff132-7e3c-444e-a980-6490e3448153",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Loading modules that we need\n",
    "from pyspark.sql.dataframe import DataFrame\n",
    "from collections import Counter\n",
    "from pyspark.sql.functions import desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b384d61-f63a-4c3f-9d87-63e0427c5ecd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# A helper function to load a table (stored in Parquet format) from DBFS as a Spark DataFrame \n",
    "def load_df(table_name: \"name of the table to load\") -> DataFrame:\n",
    "    return spark.read.parquet(table_name)\n",
    "\n",
    "users_df = load_df(\"/user/hive/warehouse/users\")\n",
    "posts_df = load_df(\"/user/hive/warehouse/posts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a34dda8-b217-4958-993e-b10e5c4c7e01",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Subtask 1: implementing two functions\n",
    "Implement these two functions:\n",
    "1. 'compute_pearsons_r' that receives a DataFrame and two column names and returns the [Pearson correlation coefficient](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient) between values of two columns;\n",
    "2. 'make_tag_graph' that in the input receives the DataFrame containing the records related to 'questions' and returns a DataFrame with two columns 'u' and 'v'; the record for row i from the resulting DataFrame is a tuple (u_i, v_i). u_i and v_j are distinct tags and have appeared together for a question.\n",
    "\n",
    "Please note that you should implement the 'compute_pearsons_r' yourself, so you should not use the 'DataFrame.stat.corr' method. Nevertheless, you can use 'DataFrame.stat.corr' to verify the correctness of your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5309637d-1430-4727-8ece-e8ce2865fae9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5218109904695661\n",
      "Before cross join:\n",
      "+---+--------+----------+------------+-----+---------+--------------------+-----------+-------------------+--------------------+----------------+-----------+------------+-------------+-------------------+\n",
      "| Id|ParentId|PostTypeId|CreationDate|Score|ViewCount|                Body|OwnerUserId|   LastActivityDate|               Title|            Tags|AnswerCount|CommentCount|FavoriteCount|          CloseDate|\n",
      "+---+--------+----------+------------+-----+---------+--------------------+-----------+-------------------+--------------------+----------------+-----------+------------+-------------+-------------------+\n",
      "|  5|    null|         1|        null|    9|      789|PHA+SSd2ZSBhbHdhe...|          5|2014-05-14 00:36:31|SG93IGNhbiBJIGRvI...|machine-learning|          1|           1|            1|2014-05-14 14:40:25|\n",
      "|  7|    null|         1|        null|    4|      459|PHA+QXMgYSByZXNlY...|         36|2014-05-16 13:45:00|V2hhdCBvcGVuLXNvd...|       education|          3|           4|            1|2014-05-14 08:40:54|\n",
      "| 14|    null|         1|        null|   25|     1845|PHA+SSBhbSBzdXJlI...|         66|2020-08-16 13:01:33|SXMgRGF0YSBTY2llb...|     data-mining|          4|           1|            6|               null|\n",
      "| 15|    null|         1|        null|    2|      648|PHA+SW4gd2hpY2ggc...|         64|2014-05-14 01:41:23|V2hhdCBhcmUgdGhlI...|       databases|          0|           1|            0|2014-05-14 07:41:49|\n",
      "| 16|    null|         1|        null|   17|      409|PHA+SSB1c2UgPGEga...|         63|2014-05-17 16:24:14|VXNlIGxpYmxpbmVhc...|machine-learning|          2|           0|            0|               null|\n",
      "| 19|    null|         1|        null|   92|    17315|PHA+TG90cyBvZiBwZ...|         84|2018-05-01 13:04:43|SG93IGJpZyBpcyBia...|         bigdata|         12|           5|           26|               null|\n",
      "| 20|    null|         1|        null|   17|      376|PHA+V2UgY3JlYXRlZ...|         96|2019-09-07 18:23:57|VGhlIGRhdGEgaW4gb...|           nosql|          5|           1|            1|               null|\n",
      "| 22|    null|         1|        null|  179|   237690|PHA+TXkgZGF0YSBzZ...|         97|2020-08-16 10:15:51|Sy1NZWFucyBjbHVzd...|     data-mining|         13|           3|          128|               null|\n",
      "| 31|    null|         1|        null|   10|     1676|PHA+SSBoYXZlIGEgY...|        118|2014-05-15 05:49:39|Q2x1c3RlcmluZyBjd...|     data-mining|          1|           4|            1|               null|\n",
      "| 35|    null|         1|        null|   19|      563|PHA+SW4gd29ya2luZ...|         26|2014-05-20 03:56:43|SG93IHRvIHNjYWxlI...|      algorithms|          3|           0|            3|               null|\n",
      "| 38|    null|         1|        null|   15|     3381|PHA+SSBoZWFyZCBhY...|        134|2015-05-18 12:30:19|V2hhdCBpcyB0aGUgZ...|           nosql|          2|           2|            2|               null|\n",
      "| 41|    null|         1|        null|   51|     8605|PHA+UiBoYXMgbWFue...|        136|2019-02-23 11:34:41|SXMgdGhlIFIgbGFuZ...|         bigdata|          9|           1|           19|               null|\n",
      "| 50|    null|         1|        null|    5|      646|PHA+SSBoYXZlIGFuI...|        151|2014-05-14 15:42:02|UnVubmluZyBhbiBSI...|               r|          2|           1|            0|2017-08-29 10:27:14|\n",
      "| 52|    null|         1|        null|   35|     4426|PHA+RnJvbSBteSBsa...|        157|2017-01-23 06:27:10|T3JnYW5pemVkIHByb...|               r|          7|           2|           19|               null|\n",
      "| 59|    null|         1|        null|   10|     1226|PHA+SW4gcmV2aWV3a...|        158|2014-07-26 15:10:51|V2hhdCBhcmUgUidzI...|   apache-hadoop|          3|           1|            1|               null|\n",
      "| 61|    null|         1|        null|   53|    14043|PHA+TG9naWMgb2Z0Z...|        158|2017-09-17 02:27:31|V2h5IElzIE92ZXJma...|machine-learning|          8|          16|           20|               null|\n",
      "| 69|    null|         1|        null|    3|       86|PHA+Rmlyc3QsIHRoa...|        158|2014-05-15 02:02:08|SXMgaXQgcG9zc2lib...|      processing|          1|           2|            0|               null|\n",
      "| 71|    null|         1|        null|   14|      674|PHA+V2hhdCBhcmUgd...|        179|2014-05-15 08:25:47|V2hlbiBhcmUgcC12Y...|         bigdata|          3|           4|            4|               null|\n",
      "| 75|    null|         1|        null|    4|      153|PHA+SWYgc21hbGwgc...|        158|2019-05-07 04:16:29|SXMgdGhlcmUgYSByZ...|      statistics|          2|           0|            1|               null|\n",
      "| 76|    null|         1|        null|    6|      174|PHA+PGVtPihOb3RlO...|        158|2014-05-18 15:18:08|V2hpY2ggQmlnIERhd...|         bigdata|          1|           6|            1|               null|\n",
      "+---+--------+----------+------------+-----+---------+--------------------+-----------+-------------------+--------------------+----------------+-----------+------------+-------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+----------------+------------------+\n",
      "|               u|                 v|\n",
      "+----------------+------------------+\n",
      "|machine-learning|       definitions|\n",
      "|machine-learning|        statistics|\n",
      "|machine-learning|       data-mining|\n",
      "|machine-learning| feature-selection|\n",
      "|machine-learning|        processing|\n",
      "|machine-learning|     apache-hadoop|\n",
      "|machine-learning|          metadata|\n",
      "|machine-learning|                 r|\n",
      "|machine-learning|               nlp|\n",
      "|machine-learning|             tools|\n",
      "|machine-learning|         databases|\n",
      "|machine-learning|       scalability|\n",
      "|machine-learning|recommender-system|\n",
      "|machine-learning|        algorithms|\n",
      "|machine-learning|  machine-learning|\n",
      "|machine-learning|       open-source|\n",
      "|machine-learning|       performance|\n",
      "|machine-learning|        clustering|\n",
      "|machine-learning|           bigdata|\n",
      "|machine-learning|         education|\n",
      "+----------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import split, explode, col, trim\n",
    "from pyspark.sql import functions as Func\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "def compute_pearsons_r(df: \"a DataFrame\", col1: \"name of column A\", col2: \"name of column B\") -> float:\n",
    "    mean1 = df.agg(Func.mean(col1)).first()[0]\n",
    "    mean2 = df.agg(Func.mean(col2)).first()[0]\n",
    "\n",
    "    squared_deviation1 = (Func.sum(Func.pow(Func.col(col1) - mean1, 2)))\n",
    "    squared_deviation2 = (Func.sum(Func.pow(Func.col(col2) - mean2, 2)))\n",
    "    value_count1 = (Func.count(Func.col(col1)) - 1)\n",
    "    value_count2 = (Func.count(Func.col(col2)) - 1)\n",
    "\n",
    "    variance1 = df.agg(squared_deviation1 / value_count1).first()[0]\n",
    "    variance2 = df.agg(squared_deviation2 / value_count2).first()[0]\n",
    "    col_product = (Func.sum(Func.col(col1) * Func.col(col2)))\n",
    "    \n",
    "    covariance = df.agg(col_product / value_count1).first()[0] - mean1 * mean2\n",
    "\n",
    "    return covariance / ((variance1 ** 0.5) * (variance2 ** 0.5))\n",
    "\n",
    "##Test\n",
    "result = compute_pearsons_r(users_df, \"Reputation\", \"UpVotes\")\n",
    "print(result)\n",
    "\n",
    "def make_tag_graph(df: \"DataFrame containing question data\") -> DataFrame:\n",
    "    questions_df = df.filter(col(\"PostTypeId\") == 1)\n",
    "\n",
    "    questions_df = questions_df.withColumn(\"Tags\", Func.regexp_replace(\"Tags\", \"<(.*?)>\", \"$1,\"))\n",
    "    questions_df = questions_df.withColumn(\"Tags\", Func.regexp_replace(\"Tags\", \">(.*?)<\", \"$1,\"))\n",
    "    print(\"After removing the < >:\")\n",
    "    questions_df.show()\n",
    "\n",
    "    questions_df = questions_df.withColumn(\"Tags\", explode(split(col(\"Tags\"), \",\")))\n",
    "    questions_df = questions_df.withColumn(\"Tags\", trim(col(\"Tags\")))\n",
    "    questions_df = questions_df.filter(col(\"Tags\").isNotNull() & (col(\"Tags\") != \"\"))\n",
    "    \n",
    "    print(\"After splitting and exploding & trimming:\")\n",
    "    questions_df.show()\n",
    "\n",
    "    result_df = (\n",
    "        questions_df.select(col(\"Tags\").alias(\"u\"))\n",
    "        .crossJoin(\n",
    "            questions_df.select(col(\"Tags\").alias(\"v\"))\n",
    "        )\n",
    "        .where((col(\"u\").isNotNull()) & (col(\"u\") != \"\") & (col(\"v\").isNotNull()) & (col(\"v\") != \"\"))\n",
    "        .distinct()\n",
    "    ).distinct()\n",
    "\n",
    "    result_df = result_df.dropDuplicates()\n",
    "\n",
    "    print(\"Data after crossjoining tags\")\n",
    "    result_df.show()\n",
    "\n",
    "    df_pandas = result_df.toPandas()\n",
    "    print(df_pandas)\n",
    "\n",
    "    return result_df\n",
    "\n",
    "result = make_tag_graph(posts_df)\n",
    "\n",
    "##Show result\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a81ba4f-9bfa-40b2-b5bb-e5797110b507",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Subtask 2: implementing three functions\n",
    "Impelment these three functions:\n",
    "1. 'get_nodes' that, given the result from execution of 'make_tag_graph', returns a DataFrame with one column named 'id' that includes the tags that have appeared in the tag graph;\n",
    "2. 'get_edges' that, given the result from execution of 'make_tag_graph', returns a DataFrame with two columns 'src' and 'dst' where 'src' is the source node and 'dst' is the destination node.\n",
    "\n",
    "\n",
    "Note that the term 'tag graph' in this context refers to the DataFrame reuturned by executing 'make_tag_graph'. Furthermore, 'src' and 'dst' are distinct, so 'src' != 'dst'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9b4472a-f1a1-46a5-bf84-a74fc5522749",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_nodes(df: \"DataFrame of the tag graph\") -> DataFrame:\n",
    "    nodes_df = df.select(col(\"u\")).union(df.select(col(\"v\"))).distinct()\n",
    "    nodes_df = nodes_df.withColumnRenamed(\"u\", \"id\")\n",
    "    return nodes_df\n",
    "  \n",
    "\n",
    "def get_edges(df: \"DataFrame of the tag graph\") -> DataFrame:\n",
    "  edges_df = df.select(col(\"u\").alias(\"src\"), col(\"v\").alias(\"dst\"))\n",
    "  #\n",
    "  return edges_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5db7dc5-c730-4c11-b5c0-f846257649e7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ipython_unittest extension is already loaded. To reload it, use:\n",
      "  %reload_ext ipython_unittest\n"
     ]
    }
   ],
   "source": [
    "# Loading 'ipython_unittest' so we can use '%%unittest_main' magic command\n",
    "%load_ext ipython_unittest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26060642-7e52-4723-98ea-3b8730753439",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Subtask 3: validating the implementation by running the tests\n",
    "\n",
    "Run the cell below and make sure that all the tests run successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0bb43bc9-1eb4-4b8d-ae5a-36f1a371688d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "application/unittest.status+json": {
       "color": "yellow",
       "message": "",
       "previous": 0
      },
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5218109904695661\n",
      "0.14735501007545418\n",
      "0.4753227874233796\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Cancelled",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%unittest_main\n",
    "class TestTask3(unittest.TestCase):\n",
    "  \n",
    "  error_threshold = 0.03\n",
    "  \n",
    "  def test_corr1(self):\n",
    "    # Pearson correlation coefficient between 'user reputation' and 'upvotes' received by users\n",
    "    result = compute_pearsons_r(users_df, \"Reputation\", \"UpVotes\")\n",
    "    self.assertLessEqual(abs(result-0.5218138310114108), self.error_threshold)\n",
    "    print(result)\n",
    "  \n",
    "  def test_corr2(self):\n",
    "    # Pearson correlation coefficient between 'user reputation' and 'downvotes' received by users\n",
    "    result = compute_pearsons_r(users_df, \"Reputation\", \"DownVotes\")\n",
    "    self.assertLessEqual(abs(result-0.1473558141546844), self.error_threshold)\n",
    "    print(result)\n",
    "\n",
    "  def test_corr3(self):\n",
    "    # Pearson correlation coefficient between 'question score' and the 'number of answers' it received\n",
    "    result = compute_pearsons_r(posts_df[posts_df[\"PostTypeId\"] == 1], \"Score\", \"AnswerCount\")\n",
    "    self.assertLessEqual(abs(result-0.47855272641249674), self.error_threshold)\n",
    "    print(result)\n",
    "    \n",
    "  def test_make_tag_graph(self):\n",
    "    result = make_tag_graph(df=posts_df[posts_df[\"PostTypeId\"] == 1])\n",
    "    self.assertIsInstance(result, DataFrame)\n",
    "    \n",
    "    coulmn_names = Counter(map(str.lower, ['u', 'v']))\n",
    "    self.assertCountEqual(coulmn_names, Counter(map(str.lower, result.columns)), \"Missing column(s) or column name mismatch\")\n",
    "    \n",
    "    display(result)\n",
    "    \n",
    "    self.assertEqual(result.count(), 228830)\n",
    "    \n",
    "  def test_get_nodes(self):\n",
    "    result = make_tag_graph(df=posts_df[posts_df[\"PostTypeId\"] == 1])\n",
    "    n = get_nodes(result)\n",
    "    self.assertEqual(n.count(), 638)\n",
    "    n.show()\n",
    "\n",
    "  def test_get_edges(self):\n",
    "    result = make_tag_graph(df=posts_df[posts_df[\"PostTypeId\"] == 1])\n",
    "    e = get_edges(result)\n",
    "    \n",
    "    coulmn_names = Counter(map(str.lower, ['src', 'dst']))\n",
    "    self.assertCountEqual(coulmn_names, Counter(map(str.lower, e.columns)), \"Missing column(s) or column name mismatch\")\n",
    "    \n",
    "    self.assertEqual(e.count(), 225290)\n",
    "    e.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89df9a4d-63c0-44e4-87d7-93602ec156a7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Subtask 4: answering to questions about Spark related concepts\n",
    "\n",
    "Please write a short description for the terms below---one to two short paragraphs for each term. Don't copy-paste; instead, write your own understanding.\n",
    "\n",
    "1. What do the terms 'User-Defined Functions (UDFs)', 'Data Locality', 'Bucketing', 'Distributed Filesystem' mean in the context of Spark?\n",
    "\n",
    "Write your descriptions in the next cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa3a29e6-80ad-4105-a8f9-dae8d991c6c6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Your answers..."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Task3",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
